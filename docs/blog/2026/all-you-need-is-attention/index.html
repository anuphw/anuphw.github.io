<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      All You Need Is, Attention! | Anup
    
  
</title>
<meta name="author" content="Anup Walvekar">
<meta name="description" content="On when attention is the right tool — and when it isn't">

  <meta name="keywords" content="deep learning, generative AI, machine learning, statistics">










<!-- Security: Content Security Policy -->
<!-- Permissive CSP suitable for academic websites. Allows external images, videos, and common embed services. -->
<!-- To customize for your needs, see: https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP -->
<meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;">

<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">



<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,600;0,700;1,400&amp;family=Source+Serif+4:ital,opsz,wght@0,8..60,300;0,8..60,400;0,8..60,600;1,8..60,400&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->




  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="https://anuphw.github.io/blog/2026/all-you-need-is-attention/">


  <!-- Dark Mode -->
  <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script>
  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>












  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          Anup
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item active">
                  <a class="nav-link" href="/blog/">blog
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/publications/">publications
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/repositories/">repositories
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/cv/">CV
                    
                  </a>
                </li>
              
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span>
              </button>
            </li>
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="fa-half-sun-moon" id="light-toggle-system"></i>
                <i class="fa-solid fa-moon" id="light-toggle-dark"></i>
                <i class="fa-solid fa-sun" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        






<div class="post">
  <header class="post-header">
    <h1 class="post-title">All You Need Is, Attention!</h1>
    <p class="post-meta">
      Created on February 25, 2026
      
      
      
      
    </p>
    <p class="post-tags">
      
        <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>
      
      
          ·  
        
          
            <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>
          
          
             
          
        
          
            <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers</a>
          
          
             
          
        
          
            <a href="/blog/tag/multimodal"> <i class="fa-solid fa-hashtag fa-sm"></i> multimodal</a>
          
          
             
          
        
          
            <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>
          
          
        
      

      
          ·  
        
          
            <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>
          
          
        
      
    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      
<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/assets/img/posts/attention-hero-480.webp 480w,/assets/img/posts/attention-hero-800.webp 800w,/assets/img/posts/attention-hero-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
      
    
    <img src="/assets/img/posts/attention-hero.png" class="img-fluid" width="100%" height="auto" alt="Two feature sets of different shapes connected by dynamic lines of light" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

<p>The title is borrowed, obviously. But the argument here is not about transformers replacing everything. It is about a narrower and more useful question: <strong>when is attention the right tool?</strong></p>

<p>Not always. But there is one class of problems where it is almost always the right answer — when you have two sets of features with different shapes that need to share information with each other. This post is about that class of problems.</p>

<hr>

<h2 id="the-problem">The Problem</h2>

<p>Suppose you are building a model that takes an image and a question about it, and produces an answer. You run the image through a CNN backbone and get a grid of feature vectors. You embed the question tokens and get a sequence of vectors. Now you need these two representations to inform each other — the image features need to know what the question is asking, and the question tokens need to know where to look in the image.</p>

<p>The trouble is that these two feature sets look nothing alike:</p>

<ul>
  <li>Image features: 49 spatial patches, each with a 512-dimensional vector → shape <code class="language-plaintext highlighter-rouge">(49 × 512)</code>
</li>
  <li>Question tokens: 12 word embeddings, each with a 256-dimensional vector → shape <code class="language-plaintext highlighter-rouge">(12 × 256)</code>
</li>
</ul>

<p>Different number of elements. Different dimensionality. No natural alignment between them.</p>

<p>How do you get them to talk to each other?</p>

<hr>

<h2 id="the-obvious-approaches-and-their-shortcomings">The Obvious Approaches, and Their Shortcomings</h2>

<p>Three approaches come to mind immediately.</p>

<p><strong>Pad and concatenate.</strong> Pad the shorter sequence to match the length of the longer one, then concatenate along the feature dimension. This forces a positional alignment that does not exist — image patch 7 has no inherent relationship to question token 7. The padding is also meaningless signal that the model has to learn to ignore.</p>

<p><strong>Global pooling.</strong> Collapse each feature set into a single fixed-size vector using average or max pooling, then concatenate or add them. This is simple and it works up to a point. But a single vector cannot represent which parts of the image are relevant to which parts of the question. You have discarded all spatial and sequential structure in the process of making the shapes compatible.</p>

<p><strong>Linear projection.</strong> Project both feature sets into a shared dimension, then combine with addition or elementwise multiplication. Better — but the projection weights are fixed after training. They do not adapt based on what the inputs actually are. A projection learned for questions about color is the same projection used for questions about location.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/assets/img/posts/attention-static-bridges-480.webp 480w,/assets/img/posts/attention-static-bridges-800.webp 800w,/assets/img/posts/attention-static-bridges-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
      
    
    <img src="/assets/img/posts/attention-static-bridges.png" class="img-fluid" width="100%" height="auto" alt="The three failing approaches — pad and concat, global pooling, linear projection" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

<p>The failure mode all three share is the same: <strong>the bridge between the two feature sets is static.</strong> It is decided at training time and does not change based on the content of the inputs at inference time.</p>

<hr>

<h2 id="what-attention-does-differently">What Attention Does Differently</h2>

<p>Attention builds a dynamic bridge. The key idea, stated in one line:</p>

\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V\]

<p>The dynamism lives in \(QK^T\) — a matrix of dot products between actual input vectors at inference time, not learned parameters. The weights that determine how much each image patch contributes to each question token are recomputed fresh for every new image-question pair. That is what makes this different from a projection. What matters here is what it does to the shapes.</p>

<p>Take the image-question example from above. Using cross-attention:</p>

<ul>
  <li>Project question tokens into queries: \(Q\) of shape <code class="language-plaintext highlighter-rouge">(12 × 64)</code>
</li>
  <li>Project image patches into keys and values: \(K\) of shape <code class="language-plaintext highlighter-rouge">(49 × 64)</code>, \(V\) of shape <code class="language-plaintext highlighter-rouge">(49 × 64)</code>
</li>
  <li>Compute \(QK^T\): shape <code class="language-plaintext highlighter-rouge">(12 × 49)</code> — each of the 12 question tokens gets a score over each of the 49 image patches</li>
  <li>After softmax and multiplying by \(V\): output shape <code class="language-plaintext highlighter-rouge">(12 × 64)</code> — same structure as the query, but now each token carries information drawn from the image patches it found most relevant</li>
</ul>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/assets/img/posts/attention-matrix-shapes-480.webp 480w,/assets/img/posts/attention-matrix-shapes-800.webp 800w,/assets/img/posts/attention-matrix-shapes-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
      
    
    <img src="/assets/img/posts/attention-matrix-shapes.png" class="img-fluid" width="100%" height="auto" alt="Matrix shape transformation through Q, K, V" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

<p>The output has the same shape as the thing asking the questions, regardless of the shape of what it is querying. The image had 49 elements; the question had 12; the output has 12 — because the question is in the driver’s seat. It gets to decide, based on its own content, how much of each image patch to pull in.</p>

<p>This is what none of the three approaches above can do.</p>

<hr>

<h2 id="when-this-actually-matters">When This Actually Matters</h2>

<p>The image-question example is a special case of a more general pattern. Here are four situations where that pattern appears, along with the models that exploited it.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/assets/img/posts/attention-use-cases-480.webp 480w,/assets/img/posts/attention-use-cases-800.webp 800w,/assets/img/posts/attention-use-cases-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
      
    
    <img src="/assets/img/posts/attention-use-cases.png" class="img-fluid" width="100%" height="auto" alt="Four use cases of attention" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

<h3 id="variable-length-sequences">Variable-Length Sequences</h3>

<p>This is the degenerate case of the pattern — self-attention, where queries and keys come from the same sequence. There is only one feature set, and it attends to itself. The “two sets with different shapes” framing still applies, just with both sets being the same sequence at the same step.</p>

<p>You have a sequence of arbitrary length — a sentence, an audio clip, a time series — and you need each element to gather context from every other element. RNNs do this by passing a fixed-size hidden state forward one step at a time; by step 100, whatever was at step 1 has been compressed through 99 nonlinearities. Long-range dependencies are difficult to preserve.</p>

<p>Attention has no concept of distance. Every token computes relevance scores against every other token in a single step, and those scores are derived from content, not position.</p>

<p><strong><a href="https://github.com/google-research/bert" rel="external nofollow noopener" target="_blank">BERT</a></strong> (Devlin et al., 2019) made this the foundation of language understanding — bidirectional self-attention over the full sequence, allowing every token to be influenced by every other token simultaneously. <strong><a href="https://arxiv.org/abs/2006.11477" rel="external nofollow noopener" target="_blank">Wav2Vec 2.0</a></strong> (Baevski et al., 2020) applied the same idea to raw audio, where the “sequence” is a stream of audio frames of arbitrary length.</p>

<h3 id="cross-modal-fusion">Cross-Modal Fusion</h3>

<p>This is the image-question case generalized. Two modalities — image and text, audio and video, sensor readings and language instructions — each with its own structure, needing to inform each other.</p>

<p>Projecting each modality to a shared space and adding them loses the relational structure between them. A model that can ask “which image regions are most relevant to this specific word?” cannot be built with a fixed projection — it needs attention.</p>

<p><strong><a href="https://github.com/openai/CLIP" rel="external nofollow noopener" target="_blank">CLIP</a></strong> (Radford et al., 2021) learns a joint embedding space for images and text using contrastive learning, with attention operating within each modality. <strong><a href="https://arxiv.org/abs/2204.14198" rel="external nofollow noopener" target="_blank">Flamingo</a></strong> (Alayrac et al., 2022) goes further, interleaving cross-attention layers that let language tokens directly attend to visual features at multiple scales. <strong><a href="https://arxiv.org/abs/2204.05862" rel="external nofollow noopener" target="_blank">DALL-E 2</a></strong> and the family of diffusion models use cross-attention to condition image generation on text embeddings, where each spatial region of the image attends to the text tokens that are relevant to it.</p>

<h3 id="irregular-spatial-structures">Irregular Spatial Structures</h3>

<p>CNNs are built around grids. Every convolution assumes that neighboring pixels are spatially close and that the input has a fixed, regular structure. This assumption breaks down for graphs, point clouds, and molecules — structures where the number of neighbors is variable and there is no canonical ordering of elements.</p>

<p>In a graph, a node needs to aggregate information from its neighbors, but different nodes have different numbers of neighbors. A point cloud representing a 3D object has no fixed spatial grid — the number of points varies, and there is no notion of “the pixel to the right.”</p>

<p>Attention handles this naturally. Given a set of elements and a way to compute pairwise relevance, it can aggregate information across any set of neighbors without requiring a fixed structure.</p>

<p><strong><a href="https://github.com/PetarV-/GAT" rel="external nofollow noopener" target="_blank">Graph Attention Networks</a></strong> (Veličković et al., 2018) apply attention over graph neighborhoods — each node attends to its neighbors, with learned weights that depend on the content of both the node and its neighbor, not just the graph topology. <strong><a href="https://github.com/google-deepmind/alphafold" rel="external nofollow noopener" target="_blank">AlphaFold2</a></strong> (Jumper et al., 2021) uses a form of attention operating over pairs of amino acid residues to propagate structural constraints through a protein’s contact map, a structure that has no fixed spatial grid.</p>

<h3 id="dynamic-feature-relationships">Dynamic Feature Relationships</h3>

<p>In standard detection and segmentation pipelines, the features used to recognize an object are determined by the architecture — a specific region proposal, a fixed anchor, a particular feature pyramid level. These are reasonable heuristics, but they are fixed.</p>

<p>Cross-attention allows the model to dynamically route information based on content. A query representing “the object I am trying to detect” can attend over the full feature map and pull in exactly the regions that are relevant — without the model needing to know in advance where those regions will be.</p>

<p><strong><a href="https://github.com/facebookresearch/detr" rel="external nofollow noopener" target="_blank">DETR</a></strong> (Carion et al., 2020) replaced the traditional detection pipeline with a set of learned object queries, each attending over the full image feature map to find the object it is responsible for. There are no anchors, no non-maximum suppression — the attention mechanism handles the assignment. In <a href="https://github.com/CompVis/latent-diffusion" rel="external nofollow noopener" target="_blank">diffusion-based image generation models</a> (Rombach et al., 2022), each spatial position in the noisy image attends to text condition tokens at every denoising step, dynamically adjusting which text tokens it draws from as the image structure emerges.</p>

<hr>

<h2 id="the-unifying-principle">The Unifying Principle</h2>

<p>Across all four cases, the same structure appears: two sets of features, different shapes, needing to exchange information in a way that depends on the content of both.</p>

<p>The common thread is that <strong>the routing of information is itself a function of the data.</strong> Which image patches are relevant to a question token depends on the question. Which graph neighbors should dominate depends on the node’s features. Which text tokens should influence a spatial position in a generated image depends on what is being generated there.</p>

<p>When that condition holds — when you cannot decide at training time how information should flow between two feature sets, because it depends on what the inputs actually are — attention is the right tool.</p>

<p>The alternative approaches all assume the routing can be fixed. Attention does not.</p>

<hr>

<h2 id="when-it-is-not-worth-it">When It Is Not Worth It</h2>

<p>Dynamic routing has a cost. The \(QK^T\) matrix has shape \((n \times m)\) where \(n\) and \(m\) are the sizes of the two feature sets. Memory and compute scale quadratically with sequence length, which means full cross-attention becomes prohibitive when both sets are large — a high-resolution image feature map attended to by a long document, for instance.</p>

<p>In practice, this is why most large-scale systems that use cross-attention introduce some form of approximation: pooling the queries down before attending, using sparse attention patterns, or chunking the sequence. The full dynamic bridge is expensive; the question is always whether the routing actually needs to be that dynamic, or whether a cheaper approximation captures most of the benefit.</p>

<p>If the relationship between two feature sets is mostly stable across inputs — the same parts of the image are almost always relevant regardless of the question — a fixed projection might close most of the gap at a fraction of the cost. Attention is the right default when you do not know that in advance.</p>

    </div>
  </article>

  

  

  
    
      



    
  

  
  
</div>

      
    </div>

    <!-- Footer -->
    



  



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>


  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script>

























  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script>






<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script>
<script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script>

<!-- Badges -->

  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>



  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
  
    <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  











  <!-- Scrolling Progress Bar -->
  <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script>







  <!-- Back to Top -->
  <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>



  <!-- Search -->
  <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script>
  <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys>
  <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script>
  <script src="/assets/js/search-data.js"></script>
  <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script>




  </body>
</html>
